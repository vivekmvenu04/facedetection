# -*- coding: utf-8 -*-
"""Titanic_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HHfaG2V13EFaasrg3WSycZs5D-tYYTRD
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load Titanic train.csv
df = pd.read_csv('train.csv')
df.head()

df.isnull().sum()/len(df)

df.info()

df['Age'] = df['Age'].fillna(df['Age'].median())
df['Cabin']=df['Cabin'].fillna(df['Cabin'].mode()[0])
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
label_encoder = LabelEncoder()
df['Sex'] = label_encoder.fit_transform(df['Sex'])
df['Embarked'] = label_encoder.fit_transform(df['Embarked'])

df.info()
df.isnull().sum()

df['Title'] = df['Name'].str.extract(r',\s*([^\.]+)\.', expand=False)
df['Title'] = label_encoder.fit_transform(df['Title'])

print(df.head(5))

#can just drop columns you dont want like cabin etc
X = df[['PassengerId', 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Title']]
y = df['Survived']


# Split and scale
from sklearn.preprocessing import StandardScaler
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier

# KNN model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# Evaluation
print(" KNN")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

from sklearn.linear_model import LogisticRegression

# Logistic Regression model
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_log = logreg.predict(X_test)

# Evaluation
print(" Logistic Regression ")
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_log))
print("Classification Report:\n", classification_report(y_test, y_pred_log))

from sklearn.cluster import KMeans
import numpy as np

# Scale entire dataset for clustering
X_scaled = StandardScaler().fit_transform(X)

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

# Align clusters with survival labels (0/1)
cluster_labels = np.where(cluster_labels == 1, 1, 0)

print("K-Means Clustering")
print("Accuracy (compared to true labels):", accuracy_score(y, cluster_labels))
print("Confusion Matrix:\n", confusion_matrix(y, cluster_labels))

from sklearn.svm import SVC

# SVM model
svm = SVC(kernel='linear', random_state=42)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

# Evaluation
print("SVM")
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)
classifier.fit(X_train, y_train)
y_pred_rf = classifier.predict(X_test)

# Evaluation
print("RandomForestClassifier")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

from sklearn.tree import DecisionTreeClassifier
Classifier=DecisionTreeClassifier(criterion='gini',max_depth=3,random_state=42)
Classifier.fit(X_train,y_train)
Y_predict=Classifier.predict(X_test)
print(accuracy_score(y_test,Y_predict))
print(confusion_matrix(y_test,Y_predict))
print(classification_report(y_test,Y_predict))